# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lz2vRO_j7r4kVjZ3R1-zIPVWUZqDlfN6
"""

import pandas as pd

#Nous allons avoir besoin de construire des objets de classe node qui vont représenter les noeuds de notre arbre
#Nous utiliserons par la suite les objets de type noeud pour construire notre arbre de décision
class Noeud:
    def __init__(self, variable = None, valeur = None, gauche = None, droite = None, profondeur = None, feuille = False):
        self.variable = variable #Correspond à la variable de sélection
        self.valeur = valeur #Correspond à la valeur de découpage
        self.gauche = gauche #Valeur de gauche
        self.droite = droite #Valeur de droite
        self.profondeur = profondeur #Profondeur du noeud
        self.feuille = feuille #Permet de savoir s'il s'agit d'une feuille de notre arbre ou non

import math
from pandas.core.dtypes.api import is_numeric_dtype
#On souhaite pouvoir construire un objet de classe arbre de décision
class DecisionTree:
    #On commence avec le constructeur de notre classe
    def __init__(self, objectif, nb_min_val=2, max_depth=10):
        #objectif désigne la variable cible que nous cherchons à déterminer à partir des autres paramètres
        #nb_min_val désigne le nombre minimum de valeurs que nous souhaitons avoir dans une feuille
        #max_depth désigne la profondeur maximum que pourra atteindre notre arbre. Ici nous mettons 10 par défaut ce qui est déjà une valeur correcte pour permettre une séparation d'un jeu assez simple
        self.objectif = objectif
        self.nb_min_val = nb_min_val
        self.max_depth = max_depth

    #Cette fonction nous permet de séparer nos données selon les valeurs qui auront été choisies
    def separation(self, variable_sep, val, data):
      #Variable_sep désigne la variable que l'on souhaite séparer
      #Val désigne la valeur de séparation pour la variable
      #data désigne notre jeu de données

      if is_numeric_dtype(data[variable_sep]):#Si la variable est quantitative
        data_gauche = data[data.loc[:, variable_sep] <= val] #On sépare selon le critère qu'on a déterminé précédement
        data_droite = data[data.loc[:, variable_sep] > val] #Le reste sera dans l'autre noeud
      else: #Si la variable est qualitative
        data_gauche = data[data.loc[:, variable_sep] == val] #On prend uniquement les valeurs qui correpondent
        data_droite = data[data.loc[:, variable_sep] != val] #Le reste va dans l'autre noeud

      return data_gauche, data_droite


    def gain_information(self, parent, enfant_g, enfant_d):
      #Pour cette partie nous avons utilisé les formules disponibles ici : https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html
        poids_gauche = len(enfant_g) / len(parent) #La proportion de valeurs du noeud de gauche
        poids_droit = len(enfant_d) / len(parent) #La proportion de valeurs du noeud de droite
        #On cherche à obtenir le meilleur gain en comparant si nous obtenons un bon résultat en calculans l'entropie
        gain = self.entropy_crit(parent) - (poids_gauche*self.entropy_crit(enfant_g) + poids_droit*self.entropy_crit(enfant_d))

        return gain

    def entropy_crit(self, jeu_valeur): #On précise sur quelle colonne on souhaite calculer le cout
        #Dans le cas ou on utilise l'entropy
        valeurs_uniques = jeu_valeur.unique() #On récupère la liste de valeurs uniques de nos données
        nb_iterations = jeu_valeur.value_counts() #On compte le nombdre d'itérations de chacune de nos valeurs

        #On calcule l'entropie avec la somme de toutes les combinaisons possibles
        entropy = -sum([nb_iterations[i]*math.log2(nb_iterations[i]) for i in range(len(valeurs_uniques))])
        return entropy #On cherche à obtenir une valeur faible pour que notre gain d'information soit élevé

    def gini_crit(self, jeu_valeur):
        #Dans le cas ou on calcule avec gini
        valeurs_uniques = jeu_valeur.unique() #On récupère la liste de valeurs uniques de nos données
        nb_iterations = jeu_valeur.value_counts() #On compte le nombdre d'itérations de chacune de nos valeurs

        #On calcule l'indicateur d'impureté de Gini à partir de la formule présente ici : https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
        gini = 1 - sum(nb_iterations[i]**2 for i in range(len(valeurs_uniques)))
        return gini #Plus cette valeur est faible et plus notre découpage est juste

    def chi_deux_crit(self, data_enfant, data_parent):
      #Dans le cas ou on calcule avec le chi deux
      #Nous avons utiliser la ressource suivante pour obtenir la formule du chi-deux : https://sefiks.com/2020/03/18/a-step-by-step-chaid-decision-tree-example/
      valeur_reelle = data_enfant.value_counts()
      valeur_attendu = data_parent.value_counts()

      chi_deux = math.sqrt((valeur_reelle[i]-valeur_attendu[i])**2/valeur_attendu[i] for i in range(len(valeur_reelle)))
      return chi_deux

    def test_meil_sep(self, data, col, type_crit = "entropy"):
      #Nous permet de calculer le meilleur score parmis toutes les valeurs possibles de notre variable
      res_sep = pd.DataFrame([], columns = ('colonne', 'valeur', 'cout')) #On construit un dataframe vide dans lequel on met nos résultats
      #liste_gauche = list()
      #liste_droite = list()

      if is_numeric_dtype(data[col]):#Si quantitatif
        #On regarde tout les doublon de valeurs possibles par ordre croissant et on calcule leur moyenne
        #De cette manière, On est certain de regarder des séparations sans avoir une unique valeur dans notre noeud
        unique_val = df.sort_values(by="Poids")[:].values

        for valeur in unique_val[:, 1]:#On rajoute 1 car la boucle commence à 0
          gauche, droite = self.separation(col, unique_val[valeur], data)

          #Selon le type de critère voulu par l'utilisateur on calcule notre cout
          if type_crit == "entropy":
            res_cout = self.gain_information(data, gauche, droite)
          elif type_crit == "gini":
            res_cout = self.gini_crit(gauche, droite)
          else:
            res_cout = self.chi_deux_crit(gauche, droite)

          #On conserve les résultats de cout pour chacune de nos valeurs dans un df
          res_sep = res_sep.append(pd.DataFrame([[valeur, res_cout]],
                                                columns = ('valeur', 'cout')))

      else:#Si qualitatif
        unique_val = data[col].unique() #On récupère toutes les valeurs uniques de notre colonne
        taille_unique = len(unique_val[0]) #On regarde le nombre de valeurs uniques que nous avons
        #print(type(unique_val))

        #On calcule ensuite le cout pour chacune des valeurs possibles pour notre variable
        for valeur in range(taille_unique+1):#On rajoute 1 car la boucle commence à 0
          gauche, droite = self.separation(col, unique_val[valeur], data)

          #Selon le type de critère voulu par l'utilisateur on calcule notre cout
          if type_crit == "entropy":
            res_cout = self.gain_information(data, gauche, droite)
          elif type_crit == "gini":
            res_cout = self.gini_crit(gauche, droite)
          else:
            res_cout = self.chi_deux_crit(gauche, droite)

          #On conserve les résultats de cout pour chacune de nos valeurs dans un df
          res_sep = res_sep.append(pd.DataFrame([[valeur, res_cout]],
                                                columns = ('valeur', 'cout')))

      #On récupère la meilleure valeur ainsi que le meilleur cout associé en prenant le cout le plus petit (ou le plus grand dans le cas du gain d'information)
      if type_crit == "entropy":
        meilleur_val, meilleur_cout = res_sep[1].max()
      else:
        meilleur_val, meilleur_cout = res_sep[1].min()

      return meilleur_val, meilleur_cout

    def meilleur_split(self, data, crit= "entropy"): #On cherche à déterminer la meilleure séparation à partir des données que nous avons
      #On construit un dataframe vide dans lequel on pourra stocker les résultats de nos séparations
      resultat = pd.DataFrame([], columns = ("Meilleure valeur", "Colonne"))

      #On enlève la colonne cible de la liste
      colonnes = data.drop(self.objectif, axis = 1)
      for colonne in colonnes: #On regarde chaque colonne de notre jeu de données
        val, cout = self.test_meil_sep(data, colonne, type_crit=crit)
        #On ajoute la valeur obtenue à notre dataframe de stockage
        resultat = resultat.append(pd.DataFrame([val, colonne], columns = ("Meilleure valeur", "Colonne")))

      #On récupère la meilleure colonne selon la valeur la plus basse que l'on a obtenu
      meilleur = resultat.loc[resultat["Meilleure valeur"] == resultat["Meilleure valeur"].min()]
      meilleur_res = meilleur[0]
      meilleur_col = meilleur[1]
      return meilleur_res, meilleur_col


    def fit(self, data, critere, profondeur = 0): #On propose une fonction fit qui nous permet de construire notre arbre selon les paramètres
    #donnés par l'utilisateur lors de la création de l'objet de class DecisionTree
    #Le nom de cette fonction correspond à celui communément utilisé par les fonctions des packages publics pour plus de facilité d'utilisation
      #On commence par définir les conditions d'arrêts
      nb_val, nb_col = data.shape

      #On vérifie donc 3 cas possibles :
      #Si toutes les valeurs de notre branche donnent le même résultat alors on peut construire notre feuille
      #Si nous avons atteint la profondeur max précisée alors on construit notre feuille
      #Si notre branche ne possède plus assez de valeurs alors on construit notre feuille
      if len(data[self.objectif].unique())==1 or profondeur == self.max_depth or nb_val<self.nb_min_val:
        return Noeud(feuille = True)

      #On sépare notre jeu de donnée selon le meilleur résultat obtenu
      meilleur_res, meilleur_col = self.meilleur_split(data, critere)
      gauche, droite = self.separation(meilleur_col, meilleur_res, data)

      #On fit de manière récursive sur les branches obtenues précédemment
      branche_gauche = self.fit(gauche, critere, profondeur + 1)
      branche_droite = self.fit(droite, critere, profondeur + 1)

      #On construit un noeud avec les informations obtenues
      return Noeud(variable = meilleur_col, valeur = meilleur_res, gauche = branche_gauche, drooite = branche_droite, profondeur = profondeur)

#Pour tester notre algorithme :
#Nous utilisons un cas simple construit à la main avec des animaux
donne_train = [
    ['Gris', 2000, True, 'Elephant'],
    ['Blanc', 500, True, 'Vache'],
    ['Rouge', 1, False, 'Poisson'],
    ['Gris', 5, True, 'Souris'],
    ['Noir', 300, True, 'Vache'],
    ['Vert', 500, True, 'Crocodile'],
]

# Nous avons ainsi 4 colonnes dont voici les labels :
header = ["Couleur", "Poids", "Vertébré", "Animal"]

#La 4e colonne est celle que nous allons chercher à déterminer à partir des 3 autres

df = pd.DataFrame(donne_train, columns= header)

print(df)

#On construit un arbre depuis notre classe
arbre = DecisionTree("Animal")

#On confirme que notre fonction de séparation fonctionne avec des valeurs qualitatives et quantitatives
gauche, droite = arbre.separation("Couleur", "Gris", df)

print(gauche)
print("---------------")
print(droite)

gauche, droite = arbre.separation("Poids", 400, df)

print(gauche)
print("---------------")
print(droite)

#Nous avons réussi à faire fonctionner l'attribut unique mais après de nouvelles modifications nous obtenons cette erreur qui ne permet plus de continuer
liste_arbre = arbre.test_meil_sep(df, "Couleur")

#Puisque la fonction test_meil_sep ne fonctionne pas notre fonctionne général fit de fonctionne pas non plus.
#Nous avons essayer de proposer un code cohérent mais nous n'avons pas résolu tout les problèmes permettant son exécution
arbre.fit(df, "entropy")

#Puisque nous n'avons pas pu faire fonctionner notre code nous avons utiliser le jeu des iris avec la fonction d'arbre de décision de sklearn pour la matrice de confusion
#Matrice de confusion
import sklearn
from sklearn import datasets

iris = datasets.load_iris()
#On construit un dataframe à partir du jeu des iris
iris_df=pd.DataFrame(iris.data)
#On ajoute le nom des colonnes ainsi que la colonne cible
iris_df.columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
iris_df = iris_df.assign(Species=iris['target'])

iris_df.describe()

#On sépare notre dataframe avec en y la variable cible
X = iris_df.drop('Species',1)
Y = iris_df.Species

from sklearn.model_selection import train_test_split

#On découpe notre jeu de données avec 25% de notre echantillon en tant que test
import random
random.seed(6)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state = 6)
print("Taille echantillon entrainement",x_train.shape)
print("Taile echantillon test",x_test.shape)

from sklearn.tree import DecisionTreeClassifier

#On utilise un arbre de décision déjà implémenté
sk_arbre = DecisionTreeClassifier()
sk_arbre.fit(x_train, y_train)
y_prediction = sk_arbre.predict(x_test)

#On obtient le score de précision de notre modèle qui est très bon
from sklearn.metrics import accuracy_score

acc = accuracy_score(y_test, y_prediction)

acc

#On affiche la matrice de confusion de nos prédictions
from sklearn.metrics import confusion_matrix

matric_conf = confusion_matrix(y_test, y_prediction)
print(matric_conf)

#On calcule le recall score
from sklearn.metrics import recall_score
rec_score = recall_score(y_test, y_prediction, average = None)

rec_score
#La meilleure valeur pour ce score est 1 ce qui veut direqu'on est très précis

#On calcule le f1 score
from sklearn.metrics import f1_score
res_f1 = f1_score(y_test, y_prediction, average = None)

res_f1
#Le meilleur score possible étant 1 on est assez bon en terme de résultat de notre arbre.